{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis, ANN, & Text Mining: Part 3 - ANN\n",
    "### Author: Michael Berbach, Isaiah Samaniego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as ff\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import and parse the Admissions dataset, and then we display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     GRE Score  TOEFL Score  University Rating  SOP  LOR  CGPA  Research  \\\n",
      "0          337          118                  4  4.5  4.5  9.65         1   \n",
      "1          324          107                  4  4.0  4.5  8.87         1   \n",
      "2          316          104                  3  3.0  3.5  8.00         1   \n",
      "3          322          110                  3  3.5  2.5  8.67         1   \n",
      "4          314          103                  2  2.0  3.0  8.21         0   \n",
      "..         ...          ...                ...  ...  ...   ...       ...   \n",
      "495        332          108                  5  4.5  4.0  9.02         1   \n",
      "496        337          117                  5  5.0  5.0  9.87         1   \n",
      "497        330          120                  5  4.5  5.0  9.56         1   \n",
      "498        312          103                  4  4.0  5.0  8.43         0   \n",
      "499        327          113                  4  4.5  4.5  9.04         0   \n",
      "\n",
      "     Chance of Admit  \n",
      "0               0.92  \n",
      "1               0.76  \n",
      "2               0.72  \n",
      "3               0.80  \n",
      "4               0.65  \n",
      "..               ...  \n",
      "495             0.87  \n",
      "496             0.96  \n",
      "497             0.93  \n",
      "498             0.73  \n",
      "499             0.84  \n",
      "\n",
      "[500 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\AppData\\Local\\Temp\\ipykernel_23004\\3656229374.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data\\Admission_Predict_Ver1.1_small_data_set_for_Linear_Regression.csv', sep=r'\\s*,\\s*')\n"
     ]
    }
   ],
   "source": [
    "# loading data set from assignment 2\n",
    "df = pd.read_csv('data\\Admission_Predict_Ver1.1_small_data_set_for_Linear_Regression.csv', sep=r'\\s*,\\s*')\n",
    "df = df.drop(columns=['Serial No.'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing ANN: Normalize Predictors and Binarize Targets\n",
    "Using the min-max approach, we rescale to the range of [0,1]. This is done by subtracting the minimum value and dividing by the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     GRE Score  TOEFL Score  University Rating    SOP    LOR      CGPA  \\\n",
      "0         0.94     0.928571               0.75  0.875  0.875  0.913462   \n",
      "1         0.68     0.535714               0.75  0.750  0.875  0.663462   \n",
      "2         0.52     0.428571               0.50  0.500  0.625  0.384615   \n",
      "3         0.64     0.642857               0.50  0.625  0.375  0.599359   \n",
      "4         0.48     0.392857               0.25  0.250  0.500  0.451923   \n",
      "..         ...          ...                ...    ...    ...       ...   \n",
      "495       0.84     0.571429               1.00  0.875  0.750  0.711538   \n",
      "496       0.94     0.892857               1.00  1.000  1.000  0.983974   \n",
      "497       0.80     1.000000               1.00  0.875  1.000  0.884615   \n",
      "498       0.44     0.392857               0.75  0.750  1.000  0.522436   \n",
      "499       0.74     0.750000               0.75  0.875  0.875  0.717949   \n",
      "\n",
      "     Research  Chance of Admit  \n",
      "0           1                1  \n",
      "1           1                1  \n",
      "2           1                0  \n",
      "3           1                1  \n",
      "4           0                0  \n",
      "..        ...              ...  \n",
      "495         1                1  \n",
      "496         1                1  \n",
      "497         1                1  \n",
      "498         0                1  \n",
      "499         0                1  \n",
      "\n",
      "[500 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "df['GRE Score'] = ((df['GRE Score'] - df['GRE Score'].min()) / (df['GRE Score'].max() - df['GRE Score'].min())).astype(np.float32)\n",
    "df['TOEFL Score'] = ((df['TOEFL Score'] - df['TOEFL Score'].min()) / (df['TOEFL Score'].max() - df['TOEFL Score'].min())).astype(np.float32)\n",
    "df['University Rating'] = ((df['University Rating'] - df['University Rating'].min()) / (df['University Rating'].max() - df['University Rating'].min())).astype(np.float32)\n",
    "df['SOP'] = ((df['SOP'] - df['SOP'].min()) / (df['SOP'].max() - df['SOP'].min())).astype(np.float32)\n",
    "df['CGPA'] = ((df['CGPA'] - df['CGPA'].min()) / (df['CGPA'].max() - df['CGPA'].min())).astype(np.float32)\n",
    "df['Research'] = ((df['Research'] - df['Research'].min()) / (df['Research'].max() - df['SOP'].min())).astype(int)\n",
    "df['LOR'] = ((df['LOR'] - df['LOR'].min()) / (df['LOR'].max() - df['LOR'].min())).astype(np.float32)\n",
    "df['Chance of Admit'] = (df['Chance of Admit'] > df['Chance of Admit'].median()).astype('int')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a test set that will take 100 rows from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.455128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.493590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.685897</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.391026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.368590</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.391026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.243590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.426282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.394231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.378205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating    SOP    LOR      CGPA  \\\n",
       "400       0.28     0.285714               0.25  0.625  0.500  0.455128   \n",
       "401       0.50     0.464286               0.25  0.500  0.500  0.493590   \n",
       "402       0.68     0.607143               0.50  0.625  0.500  0.685897   \n",
       "403       0.80     0.857143               0.75  0.750  0.625  0.778846   \n",
       "404       0.42     0.321429               0.50  0.250  0.375  0.269231   \n",
       "405       0.24     0.250000               0.50  0.375  0.500  0.208333   \n",
       "406       0.64     0.392857               0.75  0.500  0.375  0.391026   \n",
       "407       0.16     0.285714               0.50  0.375  0.750  0.368590   \n",
       "408       0.14     0.321429               0.50  0.250  0.750  0.278846   \n",
       "409       0.20     0.214286               0.00  0.250  0.375  0.391026   \n",
       "410       0.22     0.142857               0.00  0.500  0.750  0.243590   \n",
       "411       0.46     0.071429               0.25  0.375  0.125  0.426282   \n",
       "412       0.48     0.357143               0.75  0.375  0.250  0.346154   \n",
       "413       0.54     0.321429               0.50  0.500  0.250  0.365385   \n",
       "414       0.62     0.642857               0.75  0.625  0.750  0.496795   \n",
       "415       0.74     0.500000               0.75  0.750  0.875  0.625000   \n",
       "416       0.50     0.428571               0.50  0.750  0.375  0.416667   \n",
       "417       0.52     0.392857               0.50  0.625  0.250  0.282051   \n",
       "418       0.38     0.678571               0.25  0.375  0.750  0.394231   \n",
       "419       0.36     0.357143               0.25  0.250  0.625  0.378205   \n",
       "\n",
       "     Research  Chance of Admit  \n",
       "400         0                0  \n",
       "401         0                0  \n",
       "402         1                1  \n",
       "403         1                1  \n",
       "404         1                0  \n",
       "405         0                0  \n",
       "406         1                0  \n",
       "407         1                0  \n",
       "408         1                0  \n",
       "409         0                0  \n",
       "410         0                0  \n",
       "411         0                0  \n",
       "412         1                0  \n",
       "413         1                0  \n",
       "414         1                0  \n",
       "415         1                1  \n",
       "416         0                0  \n",
       "417         0                0  \n",
       "418         0                0  \n",
       "419         1                0  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 400\n",
    "test = df[index:]\n",
    "train = df[:index]\n",
    "test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.abc.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "    \n",
    "X, Y = to_xy(train,'Chance of Admit')\n",
    "testX, testY = to_xy(test, 'Chance of Admit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 7)\n",
      "(400, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\legen\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = ff.keras.Sequential()\n",
    "model.add(Dense(12, input_dim = X.shape[1], activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 - 1s - 51ms/step - loss: 0.7166\n",
      "Epoch 2/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6957\n",
      "Epoch 3/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6803\n",
      "Epoch 4/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6706\n",
      "Epoch 5/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6626\n",
      "Epoch 6/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6527\n",
      "Epoch 7/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6411\n",
      "Epoch 8/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6279\n",
      "Epoch 9/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.6133\n",
      "Epoch 10/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5987\n",
      "Epoch 11/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5829\n",
      "Epoch 12/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5659\n",
      "Epoch 13/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5480\n",
      "Epoch 14/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5315\n",
      "Epoch 15/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.5146\n",
      "Epoch 16/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4997\n",
      "Epoch 17/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4850\n",
      "Epoch 18/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4715\n",
      "Epoch 19/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4595\n",
      "Epoch 20/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4480\n",
      "Epoch 21/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4371\n",
      "Epoch 22/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4289\n",
      "Epoch 23/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4184\n",
      "Epoch 24/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4109\n",
      "Epoch 25/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.4026\n",
      "Epoch 26/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3969\n",
      "Epoch 27/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3890\n",
      "Epoch 28/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3830\n",
      "Epoch 29/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3776\n",
      "Epoch 30/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3721\n",
      "Epoch 31/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3681\n",
      "Epoch 32/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3640\n",
      "Epoch 33/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3586\n",
      "Epoch 34/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3547\n",
      "Epoch 35/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3520\n",
      "Epoch 36/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3487\n",
      "Epoch 37/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3455\n",
      "Epoch 38/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3417\n",
      "Epoch 39/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3390\n",
      "Epoch 40/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3368\n",
      "Epoch 41/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3340\n",
      "Epoch 42/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3328\n",
      "Epoch 43/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3305\n",
      "Epoch 44/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3285\n",
      "Epoch 45/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3282\n",
      "Epoch 46/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3253\n",
      "Epoch 47/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3235\n",
      "Epoch 48/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3227\n",
      "Epoch 49/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3209\n",
      "Epoch 50/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3183\n",
      "Epoch 51/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3175\n",
      "Epoch 52/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3169\n",
      "Epoch 53/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3177\n",
      "Epoch 54/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3141\n",
      "Epoch 55/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3134\n",
      "Epoch 56/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3117\n",
      "Epoch 57/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3118\n",
      "Epoch 58/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3111\n",
      "Epoch 59/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3089\n",
      "Epoch 60/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3084\n",
      "Epoch 61/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3090\n",
      "Epoch 62/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3068\n",
      "Epoch 63/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3061\n",
      "Epoch 64/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3083\n",
      "Epoch 65/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3045\n",
      "Epoch 66/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3045\n",
      "Epoch 67/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3030\n",
      "Epoch 68/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3038\n",
      "Epoch 69/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3009\n",
      "Epoch 70/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3004\n",
      "Epoch 71/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2991\n",
      "Epoch 72/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2998\n",
      "Epoch 73/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2975\n",
      "Epoch 74/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2974\n",
      "Epoch 75/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.3019\n",
      "Epoch 76/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2951\n",
      "Epoch 77/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2972\n",
      "Epoch 78/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2944\n",
      "Epoch 79/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2947\n",
      "Epoch 80/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2951\n",
      "Epoch 81/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2927\n",
      "Epoch 82/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2937\n",
      "Epoch 83/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2930\n",
      "Epoch 84/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2946\n",
      "Epoch 85/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2950\n",
      "Epoch 86/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2938\n",
      "Epoch 87/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2900\n",
      "Epoch 88/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2906\n",
      "Epoch 89/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2905\n",
      "Epoch 90/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2897\n",
      "Epoch 91/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2893\n",
      "Epoch 92/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2883\n",
      "Epoch 93/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2887\n",
      "Epoch 94/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2882\n",
      "Epoch 95/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2878\n",
      "Epoch 96/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2905\n",
      "Epoch 97/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2868\n",
      "Epoch 98/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2893\n",
      "Epoch 99/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2890\n",
      "Epoch 100/100\n",
      "13/13 - 0s - 2ms/step - loss: 0.2866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2003f151950>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model.fit(X,Y,verbose=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "[0.9360205  0.06397951]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(testX)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.argmax(testY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  [array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int64)]\n",
      "True:  [array([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted: \", [pred])\n",
    "print(\"True: \", [true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate accuracy from the ANN and classification report, we can see that our predicted values are above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data is 0.90\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on test data is %.2f' % (accuracy_score(true, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        56\n",
      "           1       0.89      0.89      0.89        44\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
